# Core dependencies
torch>=2.0.0
transformers>=4.38.0
datasets>=2.16.0
peft>=0.8.0

# Training and logging
wandb>=0.16.0
tqdm>=4.66.0

# Configuration
PyYAML>=6.0

# Utilities
numpy>=1.24.0
pandas>=2.0.0

# LLM evaluation (optional, for LLM-as-judge)
anthropic>=0.25.0

# Optional: for quantization
bitsandbytes>=0.42.0

# Optional: for better attention implementations
flash-attn>=2.5.0  # Requires CUDA and specific GPU support